import httpx, asyncio
import json
import re
from app.services.logs import logger
from app.config import RANGE, SPREADSHEET_ID, API_KEY


# üî• –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–≤
class SizeNormalizer:
    def __init__(self):
        self.numeric_to_letter = {
            "42": "XS", "44": "S", "46": "S", "48": "M",
            "50": "L", "52": "XL", "54": "XXL", "56": "XXXL",
            "58": "XXXXL", "60": "XXXXXL"
        }
        self.letter_variants = {
            "XS": ["xs", "x—Å", "—Ö—Å"],
            "S": ["s", "c", "—Å"],
            "M": ["m", "–º"],
            "L": ["l", "–ª"],
            "XL": ["xl", "—Ö–ª", "x–ª"],
            "XXL": ["xxl", "2xl", "2—Ö–ª", "—Ö—Ö–ª"],
            "XXXL": ["xxxl", "3xl", "3—Ö–ª", "—Ö—Ö—Ö–ª"],
            "XXXXL": ["xxxxl", "4xl", "4—Ö–ª", "—Ö—Ö—Ö—Ö–ª"],
            "XXXXXL": ["xxxxxl", "5xl", "5—Ö–ª", "—Ö—Ö—Ö—Ö—Ö–ª"]
        }

    async def normalize(self, size_str: str) -> str:
        if not size_str:
            return ""
        s = str(size_str).strip().lower()

        # –∑–∞–º–µ–Ω—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü—É
        replacements = {"—Å": "s", "—Ö": "x", "–ª": "l", "–º": "m"}
        for old, new in replacements.items():
            s = s.replace(old, new)

        # –±—É–∫–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        for letter, variants in self.letter_variants.items():
            if s in variants or s == letter.lower():
                return letter

        # —á–∏—Å–ª–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        numbers = re.findall(r"\d+", s)
        if numbers:
            num = numbers[0]
            if num in self.numeric_to_letter:
                return self.numeric_to_letter[num]

        # EU/IT —Ñ–æ—Ä–º–∞—Ç—ã
        if s.startswith(("eu", "it")):
            digits = re.sub(r"\D", "", s)
            if digits in self.numeric_to_letter:
                return self.numeric_to_letter[digits]

        return str(size_str).upper()


size_normalizer = SizeNormalizer()


# --- —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–µ—Ä–∞ ---
async def extract_ad_id_from_url(ad_url: str):
    logger.info(f"[Parser] –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
    if not ad_url:
        return None
    match = re.search(r'_(\d+)$', ad_url)
    if match:
        return match.group(1)
    match = re.search(r'/(\d{7,})(?:/|$)', ad_url)
    if match:
        return match.group(1)
    return None


async def parse_ids_from_cell(cell_value):
    if not cell_value:
        return []
    cell_value = str(cell_value).strip()
    ids = re.split(r'[,\s]+', cell_value)
    return [id_str.strip() for id_str in ids if id_str.strip().isdigit()]


async def get_all_sheet_names():
    logger.info(f"[Parser] –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ª–∏—Å—Ç–æ–≤ —Å –≥—É–≥–ª —Ç–∞–±–ª–∏—Ü—ã")
    url = f"https://sheets.googleapis.com/v4/spreadsheets/{SPREADSHEET_ID}?key={API_KEY}"

    max_retries = 2
    for attempt in range(max_retries):
        async with httpx.AsyncClient() as client:
            try:
                if attempt > 0:
                    logger.info(f"[Parser] –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                    await asyncio.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º

                response = await client.get(url)
                response.raise_for_status()
                data = response.json()

                sheet_names = [s['properties']['title'] for s in data.get('sheets', [])]
                logger.info(f"[Parser] –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω —Å–ø–∏—Å–æ–∫ –ª–∏—Å—Ç–æ–≤: {sheet_names}")
                return sheet_names

            except Exception as e:
                logger.error(
                    f"[Parser] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –ª–∏—Å—Ç–æ–≤ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {type(e).__name__}: {str(e)}")
                logger.error(f"[Parser] URL: {url}")

                # –ï—Å–ª–∏ —ç—Ç–æ HTTP –æ—à–∏–±–∫–∞, –ª–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–¥ –∏ –æ—Ç–≤–µ—Ç
                if hasattr(e, 'response'):
                    logger.error(f"[Parser] Status code: {e.response.status_code}")
                    logger.error(f"[Parser] Response text: {e.response.text}")

                # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                if attempt == max_retries - 1:
                    logger.error(f"[Parser] –ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ª–∏—Å—Ç–æ–≤")
                    return []


async def search_product_in_sheet(ad_id: str, sheet_name: str):
    """
    –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å –Ω—É–∂–Ω—ã–º ID –Ω–∞ –ª–∏—Å—Ç–µ. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å –º–∞—Å—Å–∏–≤ rows –∏ –∏–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏.
    –õ–æ–∫–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å—Å—è –ø–æ–∑–∂–µ (–ø–æ–¥–Ω–∏–º–∞—è—Å—å –≤–≤–µ—Ä—Ö –¥–æ –±–ª–∏–∂–∞–π—à–µ–π ¬´—à–∞–ø–∫–∏¬ª).
    """
    logger.info(f"[Parser] –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–∞ —Å ID {ad_id} –≤ –ª–∏—Å—Ç–µ: {sheet_name}")
    url = f"https://sheets.googleapis.com/v4/spreadsheets/{SPREADSHEET_ID}/values/{sheet_name}!{RANGE}?majorDimension=ROWS&key={API_KEY}"

    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(url)
            response.raise_for_status()
            data = response.json()

            if "values" not in data or not data["values"]:
                return None

            rows = data["values"]

            # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é ¬´—à–∞–ø–∫—É¬ª –ª–∏—Å—Ç–∞ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è)
            headers_row_index = None
            for i, row in enumerate(rows):
                if row and isinstance(row[0], str) and row[0].strip().lower() in ["id", "–∏–¥", "–∞—Ä—Ç–∏–∫—É–ª", "–∫–æ–¥"]:
                    headers_row_index = i
                    break

            start_idx = headers_row_index + 1 if headers_row_index is not None else 0
            found_row_index = None

            for i in range(start_idx, len(rows)):
                row = rows[i]
                if not row:
                    continue
                first_cell = row[0] if len(row) > 0 else ""
                ids_in_cell = await parse_ids_from_cell(first_cell)
                if ad_id in ids_in_cell:
                    found_row_index = i
                    break

            if found_row_index is None:
                logger.info(f"[Parser] –¢–æ–≤–∞—Ä —Å ID {ad_id} –≤ –ª–∏—Å—Ç–µ {sheet_name} –ù–ï –ù–ê–ô–î–ï–ù")
                return None

            return {
                'sheet_name': sheet_name,
                'found_row_index': found_row_index,
                'rows': rows
            }

        except httpx.HTTPStatusError as e:
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ HTTP –æ—à–∏–±–æ–∫
            logger.error(f"[Parser] HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –ª–∏—Å—Ç–µ {sheet_name}: "
                         f"–°—Ç–∞—Ç—É—Å {e.response.status_code}, "
                         f"–û—Ç–≤–µ—Ç: {e.response.text}")
            return None

        except httpx.RequestError as e:
            # –û—à–∏–±–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, —Ç–∞–π–º–∞—É—Ç—ã –∏ —Ç.–¥.
            logger.error(f"[Parser] –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –ª–∏—Å—Ç–µ {sheet_name}: "
                         f"{type(e).__name__}: {str(e)}")
            return None

        except KeyError as e:
            # –û—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–ª—é—á–∞–º –≤ JSON
            logger.error(f"[Parser] –û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç–µ {sheet_name}: "
                         f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á {e}")
            logger.error(f"[Parser] –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {data if 'data' in locals() else '–¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}")
            return None

        except Exception as e:
            # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ —Å –ø–æ–ª–Ω—ã–º —Å—Ç–µ–∫–æ–º –≤—ã–∑–æ–≤–æ–≤
            logger.error(f"[Parser] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –ª–∏—Å—Ç–µ {sheet_name}: "
                         f"{type(e).__name__}: {str(e)}")
            return None


async def fetch_google_sheet_stock(ad_url: str):
    ad_id = await extract_ad_id_from_url(ad_url)
    if not ad_id:
        return None
    sheet_names = await get_all_sheet_names()
    for sheet_name in sheet_names:
        if sheet_name.lower() == 'knowledge_base':
            continue
        result = await search_product_in_sheet(ad_id, sheet_name)
        if result:
            return await parse_product_from_sheet_data(result, ad_id)
    return None


async def _find_local_header_index(rows, start_from_index: int):
    header_markers = {"id", "–∏–¥", "–∞—Ä—Ç–∏–∫—É–ª", "–∫–æ–¥"}
    i = start_from_index
    while i >= 0:
        row = rows[i] if i < len(rows) else []
        first_cell = (row[0].strip().lower() if row and isinstance(row[0], str) else "")
        if first_cell in header_markers:
            return i
        i -= 1
    return None


async def _is_new_header_row(row) -> bool:
    if not row:
        return False
    first_cell = (row[0].strip().lower() if isinstance(row[0], str) else "")
    return first_cell in {"id", "–∏–¥", "–∞—Ä—Ç–∏–∫—É–ª", "–∫–æ–¥"}


async def parse_product_from_sheet_data(sheet_data, ad_id: str):
    """
    –î–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏:
      1) –ø–æ–¥–Ω–∏–º–∞–µ–º—Å—è –≤–≤–µ—Ä—Ö –¥–æ –±–ª–∏–∂–∞–π—à–µ–π —Å—Ç—Ä–æ–∫–∏-—à–∞–ø–∫–∏ (id/–∏–¥/–∞—Ä—Ç–∏–∫—É–ª/–∫–æ–¥),
      2) –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏,
      3) —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –±–ª–æ–∫–∞ –¥–æ —Å–ª–µ–¥—É—é—â–µ–π —à–∞–ø–∫–∏/–ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏/—Å—Ç—Ä–æ–∫–∏ –¥—Ä—É–≥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞,
      4) –ø–∞—Ä—Å–∏–º —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
    """
    logger.info("[Parser] –ü–∞—Ä—Å–∏–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–æ–≤–∞—Ä —Å –ª–∏—Å—Ç–∞")
    try:
        sheet_name = sheet_data['sheet_name']
        rows = sheet_data['rows']
        found_row_index = sheet_data['found_row_index']

        # 1) –õ–æ–∫–∞–ª—å–Ω–∞—è —à–∞–ø–∫–∞
        header_row_index = await _find_local_header_index(rows, found_row_index - 1)
        if header_row_index is None:
            return None

        local_headers = rows[header_row_index]

        # 2) –°–æ–±–∏—Ä–∞–µ–º –±–ª–æ–∫ —Å—Ç—Ä–æ–∫
        product_rows = []
        logger.info("[Parser] –ü–∞—Ä—Å–∏–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ —è—á–µ–π–∫–∏")
        found_ids = await parse_ids_from_cell(rows[found_row_index][0]) if rows[found_row_index] else []
        current_index = found_row_index

        while current_index < len(rows):
            row = rows[current_index]
            if not row:
                break

            if await _is_new_header_row(row) and current_index != found_row_index:
                break

            if current_index > found_row_index and len(row) > 0 and row[0]:
                logger.info("[Parser] –ü–∞—Ä—Å–∏–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∏–∑ —è—á–µ–π–∫–∏")
                current_ids = await parse_ids_from_cell(row[0])
                if current_ids and not any(_id in found_ids for _id in current_ids):
                    break

            if len(row) > 3:
                product_rows.append(row)

            current_index += 1

        if not product_rows:
            return None

        return await parse_stock_with_all_info(local_headers, product_rows, ad_id, sheet_name)

    except Exception as e:
        logger.error(f"[Parser] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ª–∏—Å—Ç–∞: {e}")
        return None


async def parse_stock_with_all_info(headers, product_rows, ad_id: str, category: str):
    try:
        async def safe_get(lst, index, default=''):
            try:
                value = lst[index] if index < len(lst) else default
                if value is None:
                    return default
                return value.strip() if isinstance(value, str) else value
            except (IndexError, TypeError):
                return default

        header_lower = [(h.lower().strip() if isinstance(h, str) else "") for h in headers]

        name_col = 1 if len(headers) > 1 else None
        price_col = 2 if len(headers) > 2 else None

        color_col = None
        for i, h in enumerate(header_lower):
            if h and ("—Ü–≤–µ—Ç" in h or "color" in h):
                color_col = i
                break
        if color_col is None:
            color_col = 3 if len(headers) > 3 else None

        description_col = None
        size_info_col = None
        for i, h in enumerate(header_lower):
            if not h:
                continue
            if description_col is None and ("–æ–ø–∏—Å–∞–Ω–∏–µ" in h or "description" in h):
                description_col = i
            elif size_info_col is None and ("—Ä–∞–∑–º–µ—Ä" in h and "–æ–ø–∏—Å–∞–Ω" not in h):
                size_info_col = i

        base_idx = (color_col + 1) if color_col is not None else ((price_col + 1) if price_col is not None else 4)

        size_columns = {}
        for i in range(base_idx, len(headers)):
            header_text = headers[i].strip() if i < len(headers) and isinstance(headers[i], str) else ""
            if not header_text:
                continue
            normalized = await size_normalizer.normalize(header_text)
            if normalized in ['XS', 'S', 'M', 'L', 'XL', 'XXL', 'XXXL', 'XXXXL', 'XXXXXL'] or re.search(r'\d+', normalized):
                size_columns[i] = normalized

        # Photo column
        photo_col = None
        photo_pattern = re.compile(r'(—Ñ–æ—Ç–æ|photo|image|pic)', re.IGNORECASE)
        photo_id_pattern = re.compile(r'(?:—Ñ–æ—Ç–æ|photo|image|pic)[ _-]*id|id[ _-]*(?:—Ñ–æ—Ç–æ|photo|image|pic)', re.IGNORECASE)
        for i in range(base_idx, len(headers)):
            h = header_lower[i]
            if not h or h == "id":
                continue
            if photo_pattern.search(h) or photo_id_pattern.search(h):
                photo_col = i
                break
        if photo_col is None and size_columns:
            last_size_idx = max(size_columns.keys())
            candidate = last_size_idx + 1
            if candidate < len(headers):
                cand_header = header_lower[candidate]
                if "–æ–ø–∏—Å–∞–Ω–∏–µ" in cand_header or "description" in cand_header:
                    if candidate + 1 < len(headers):
                        candidate += 1
                photo_col = candidate

        first_row = product_rows[0]
        product = {
            'id': ad_id,   # üëà —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã!
            'category': category,
            'name': await safe_get(first_row, name_col) if name_col is not None else '',
            'price': await safe_get(first_row, price_col, '') if price_col is not None else '',
            'description': await safe_get(first_row, description_col) if description_col is not None else '',
            'size_info': await safe_get(first_row, size_info_col) if size_info_col is not None else '',
            'payment_method': '',
            'delivery_method': '',
            'photo_ids': '',
            'stock': []
        }

        if photo_col is not None:
            photos = []
            for row in product_rows:
                val = await safe_get(row, photo_col, '')
                if val:
                    photos.append(str(val).strip())
            if photos:
                uniq = []
                seen = set()
                for p in photos:
                    if p not in seen:
                        seen.add(p)
                        uniq.append(p)
                product['photo_ids'] = ", ".join(uniq)

        for row in product_rows:
            color_val = await safe_get(row, color_col) if color_col is not None else ''
            if not color_val:
                continue
            stock_item = {'color': color_val, 'sizes': {}}
            for col_index, size_name in size_columns.items():
                quantity_raw = await safe_get(row, col_index, '0')
                try:
                    quantity = int(quantity_raw) if str(quantity_raw).isdigit() else 0
                except ValueError:
                    quantity = 0
                stock_item['sizes'][size_name] = "–ï—Å—Ç—å –≤ –Ω–∞–ª–∏—á–∏–∏" if quantity > 0 else "–ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏"
            stock_item['has_available_sizes'] = any(v == "–ï—Å—Ç—å –≤ –Ω–∞–ª–∏—á–∏–∏" for v in stock_item['sizes'].values())
            product['stock'].append(stock_item)

        product['has_stock'] = any(item['has_available_sizes'] for item in product['stock'])
        product['available_colors'] = [item['color'] for item in product['stock'] if item['has_available_sizes']]

        json_result = json.dumps(product, ensure_ascii=False, indent=4)
        print("=== JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–µ—Ä–∞ ===")
        print(json_result)
        print("=== –ö–æ–Ω–µ—Ü JSON ===")

        return json_result

    except Exception as e:
        logger.error(f'[Parser] –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}')
        return None


async def get_knowledge_base():
    knowledge_sheet_names = ['knowledge_base']
    for sheet_name in knowledge_sheet_names:
        url = f"https://sheets.googleapis.com/v4/spreadsheets/{SPREADSHEET_ID}/values/{sheet_name}!{RANGE}?majorDimension=ROWS&key={API_KEY}"
        async with httpx.AsyncClient() as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
                data = response.json()
                if "values" not in data or not data["values"]:
                    continue
                result = []
                for row in data["values"]:
                    if len(row) >= 2:
                        result.append({'question': row[0], 'answer_example': row[1]})
                    elif len(row) == 1:
                        result.append({'question': row[0], 'answer_example': ''})
                return json.dumps(result, ensure_ascii=False, indent=2)
            except Exception as e:
                logger.error(f"[Parser] –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –ª–∏—Å—Ç–∞ {sheet_name}: {e}")
                continue
    return None
